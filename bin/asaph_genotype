#!/usr/bin/env python3
"""
Command-line tool for performing unsupervised genotyping of genetic samples using clustering algorithms.
This tool provides functionality for testing principal components against known labels, clustering samples
to infer genotypes, sweeping clustering parameters to find optimal configurations, and evaluating predicted
genotypes against ground truth labels. It supports both k-means and DBSCAN clustering methods with various
parameter optimization strategies for genotype inference from genetic variation data.

Copyright 2019 Ronald J. Nowling

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import argparse
from collections import defaultdict
import itertools
import os
import sys
import warnings

import numpy as np
from scipy import stats

from sklearn.cluster import dbscan
from sklearn.cluster import k_means
from sklearn.metrics import accuracy_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier

def read_pca_coordinates(flname):
    if not os.path.exists(flname):
        print("Coordinates file path is invalid")
        sys.exit(1)

    sample_coordinates = []
    sample_names = []
    with open(flname, "rt", encoding="utf-8") as fl:
        # skip header
        next(fl)
        for ln in fl:
            cols = ln.split("\t")

            sample_name = cols[0]
            coordinates = list(map(float, cols[1:]))

            sample_names.append(sample_name)
            sample_coordinates.append(coordinates)

    coordinates = np.array(sample_coordinates)

    return sample_names, coordinates

def read_labels(flname):
    sample_indices = dict()

    with open(flname, "rt", encoding="utf-8") as fl:
        for label_idx, ln in enumerate(fl):
            cols = ln.strip().split(",")

            for sample_name in cols[1:]:
                sample_indices[sample_name] = label_idx

    return sample_indices

def read_label_names(flname):
    sample_indices = dict()

    with open(flname, "rt", encoding="utf-8") as fl:
        for ln in fl:
            cols = ln.strip().split(",")

            label = cols[0]

            for sample_name in cols[1:]:
                sample_indices[sample_name] = label

    return sample_indices

def test_pcs(coordinates, sample_names, sample_labels):
    for i in range(coordinates.shape[1]):
        feature_to_coords = defaultdict(list)
        for j, name in enumerate(sample_names):
            label = sample_labels[name]
            feature_to_coords[label].append(coordinates[j, i])

        if len(feature_to_coords) < 2:
            pvalue = 1.0
        else:
            _, pvalue = stats.f_oneway(*feature_to_coords.values())

            if np.isnan(pvalue) or np.isinf(pvalue):
                pvalue = 1.0

        print("Component:", (i+1))
        print("p-value: ", pvalue)
        print()

def cluster_samples(coordinates, sample_names, components, n_clusters, scale_features, output_fl):
    components = list(map(lambda idx: idx - 1, components))

    selected = coordinates[:, components]

    if scale_features:
        selected = StandardScaler().fit_transform(selected)

    _, cluster_idx, _ = k_means(selected, n_clusters)

    # group samples by cluster
    populations = defaultdict(set)
    outliers = []
    for i, sample_name in enumerate(sample_names):
        cluster_assignment = cluster_idx[i]
        populations[cluster_assignment].add(sample_name)

        # find outliers
        if cluster_idx[i] == -1:
            outliers.append(sample_name)

    if len(outliers) > 0:
        print("The following samples were marked as outliers:", ",".join(outliers))

    if len(populations) == 0:
        warnings.warn("All samples were marked as outliers!", UserWarning)
    else:
        print("Found", len(populations), "clusters (including outliers)")

    with open(output_fl, "wt", encoding="utf-8") as fl:
        for pop_name, samples in populations.items():
            fl.write(str(pop_name))
            for name in samples:
                fl.write(",")
                fl.write(name)
            fl.write("\n")

def generate_kmeans_clusterings(coordinates, components, n_clusters):
    for n_components in range(1, len(components) + 1):
        for selected in itertools.combinations(components, n_components):
            selected = list(sorted(selected))
            selected_out = list(map(lambda c: c + 1, selected))
            selected_coordinates = coordinates[:, selected]
            for k in n_clusters:
                for scaling in [False, True]:
                    if scaling:
                        selected_coordinates = StandardScaler().fit_transform(selected_coordinates)
                    centroids, cluster_idx, _ = k_means(selected_coordinates, k)

                    params = { "n_clusters" : k,
                               "components" : selected_out,
                               "feature_scaling" : False }

                    yield cluster_idx, centroids, params


def sweep_kmeans_parameters(coordinates, sample_names, known_labels, components, n_clusters):
    components = list(map(lambda idx: idx - 1, components))

    gen = generate_kmeans_clusterings(coordinates, components, n_clusters)
    best_score = (-1, 1000)
    best_params = None
    best_centroids = None
    for cluster_idx, centroids, params in gen:

        cluster_labels = dict()
        for i, sample_name in enumerate(sample_names):
            cluster_labels[sample_name] = cluster_idx[i]

        score = evaluate_clustering(cluster_labels,
                                    known_labels)

        print(params, score)

        # given two equal scores, prefer
        # the parameters with fewer components
        score = (score, -len(params["components"]))
        if score > best_score:
            best_score = score
            best_params = params
            best_centroids = centroids

    print("Best score:", best_score[0])
    print("Best parameters:", best_params)
    print("Best centroids:", best_centroids)

def generate_dbscan_clusterings(coordinates, components, eps_range, min_samples_range):
    for n_components in range(1, len(components) + 1):
        for selected in itertools.combinations(components, n_components):
            selected = list(sorted(selected))
            selected_out = list(map(lambda c: c + 1, selected))
            selected_coordinates = coordinates[:, selected]
            for eps in np.arange(*eps_range):
                for min_samples in range(*min_samples_range):
                    for scaling in [False, True]:
                        if scaling:
                            selected_coordinates = StandardScaler().fit_transform(selected_coordinates)
                        _, cluster_idx = dbscan(selected_coordinates, eps=eps, min_samples=min_samples)

                    params = { "eps" : eps,
                               "min_samples" : min_samples,
                               "components" : selected_out,
                               "feature_scaling" : False }

                    yield cluster_idx, params

def sweep_dbscan_parameters(coordinates, sample_names, known_labels, components, eps_range, min_samples_range):
    components = list(map(lambda idx: idx - 1, components))

    gen = generate_dbscan_clusterings(coordinates, components, eps_range, min_samples_range)
    best_score = (-1, 1000)
    best_params = None
    for cluster_idx, params in gen:

        cluster_labels = dict()
        for i, sample_name in enumerate(sample_names):
            cluster_labels[sample_name] = cluster_idx[i]

        # degenerate solution
        if len(set(cluster_idx)) == 1:
            continue

        score = evaluate_clustering(cluster_labels,
                                    known_labels)

        print(params, score)

        # given two equal scores, prefer
        # the parameters with fewer components
        score = (score, -len(params["components"]))
        if score > best_score:
            best_score = score
            best_params = params

    print("Best score:", best_score[0])
    print("Best parameters:", best_params)

def evaluate_clustering(cluster_labels, known_labels):
    score1 = evaluate_clustering_one_way(cluster_labels, known_labels)
    score2 = evaluate_clustering_one_way(known_labels, cluster_labels)
    metric = (score1 + score2) / 2.0
    return metric

def evaluate_clustering_one_way(cluster_labels, known_labels):
    # dump points marked as outliers
    outliers = { name for name in cluster_labels.keys() if cluster_labels[name] == -1 }
    common_names = set(cluster_labels.keys()) & set(known_labels.keys()) - outliers

    cluster_labels = { name : cluster_labels[name] for name in common_names }
    other_labels = { name : known_labels[name] for name in common_names }

    feature_encoder = OneHotEncoder(sparse_output=False)
    cluster_features = np.array(list(cluster_labels.values())).reshape(-1, 1)
    features = feature_encoder.fit_transform(cluster_features)

    label_encoder = LabelEncoder()
    sample_labels = label_encoder.fit_transform(list(other_labels.values()))

    dt = DecisionTreeClassifier()
    dt.fit(features, sample_labels)

    pred_labels = dt.predict(features)

    return balanced_accuracy_score(sample_labels, pred_labels)

def evaluate_predictions(cluster_labels_fl, other_labels_fl):
    orig_cluster_labels = read_label_names(cluster_labels_fl)
    known_labels = read_label_names(other_labels_fl)

    common_names = set(orig_cluster_labels.keys()) & set(known_labels.keys())

    print(len(orig_cluster_labels), len(known_labels), len(common_names))

    cluster_labels = { name : orig_cluster_labels[name] for name in common_names if orig_cluster_labels[name] != "-1" }
    cluster_outlier_labels = { name : orig_cluster_labels[name] for name in common_names if orig_cluster_labels[name] == "-1" }
    other_labels = { name : known_labels[name] for name in cluster_labels }
    other_outlier_labels = { name : known_labels[name] for name in cluster_outlier_labels }

    feature_encoder = OneHotEncoder(sparse_output=False)
    cluster_features = np.array(list(cluster_labels.values())).reshape(-1, 1)
    features = feature_encoder.fit_transform(cluster_features)

    print(features)

    label_encoder = LabelEncoder()
    label_encoder.fit(list(other_labels.values()) + list(other_outlier_labels.values()))
    sample_labels = label_encoder.transform(list(other_labels.values()))

    print(sample_labels)

    dt = DecisionTreeClassifier()
    dt.fit(features, sample_labels)

    pred_labels = dt.predict(features)

    if len(cluster_outlier_labels) != 0:
        outlier_pred_labels = -1 * np.ones(len(cluster_outlier_labels), dtype=np.int)
        sample_outlier_labels = label_encoder.transform(np.array(list(other_outlier_labels.values())))

        pred_labels = np.concatenate([pred_labels,
                                      outlier_pred_labels])

        sample_labels = np.concatenate([sample_labels,
                                        sample_outlier_labels])

    acc = accuracy_score(sample_labels, pred_labels)
    balanced = balanced_accuracy_score(sample_labels, pred_labels)

    print(pred_labels)
    print()
    print(sample_labels)

    print("Classifier accuracy:", "%.01f%%" % (100. * acc))
    print("Classifier balanced accuracy:", "%.01f%%" % (100. * balanced))
    print("Confusion matrix:")

    all_labels = list(set(pred_labels) | set(sample_labels))
    all_labels.sort()

    print("Labels:", all_labels)
    print(confusion_matrix(pred_labels, sample_labels))

def parseargs():
    parser = argparse.ArgumentParser()

    subparsers = parser.add_subparsers(dest="mode", required=True)

    label_test_parser = subparsers.add_parser("test-pcs",
                                              help="Run association tests on PCs vs labels")

    label_test_parser.add_argument("--workdir",
                                   type=str,
                                   required=True)

    label_test_parser.add_argument("--labels-fl",
                                   type=str,
                                   required=True,
                                   help="Labels file")

    cluster_parser = subparsers.add_parser("cluster",
                                           help="Infer genotypes with clustering")

    cluster_parser.add_argument("--workdir",
                                     type=str,
                                     required=True)

    cluster_parser.add_argument("--components",
                                type=int,
                                nargs="+",
                                required=True,
                                help="Components to use in projection")

    cluster_parser.add_argument("--n-clusters",
                                type=int,
                                required=True,
                                help="Number of clusters")

    cluster_parser.add_argument("--scale-features",
                                action="store_true")

    cluster_parser.add_argument("--predicted-labels-fl",
                                type=str,
                                required=True,
                                help="Predicted labels are written to this file")

    sweep_parser = subparsers.add_parser("sweep-parameters",
                                         help="Identify optimal parameters to match known genotypes")

    sweep_parser.add_argument("--workdir",
                              type=str,
                              required=True)

    sweep_parser.add_argument("--components",
                              type=int,
                              nargs="+",
                              required=True,
                              help="Components to test")

    sweep_parser.add_argument("--labels-fl",
                              type=str,
                              required=True,
                              help="Ground truth labels")

    sweep_subparsers = sweep_parser.add_subparsers(dest="sweep_mode", required=True)

    sweep_kmeans = sweep_subparsers.add_parser("kmeans")

    sweep_kmeans.add_argument("--n-clusters",
                              type=int,
                              required=True,
                              nargs="+",
                              help="Number of clusters to test")

    sweep_dbscan = sweep_subparsers.add_parser("dbscan")

    sweep_dbscan.add_argument("--eps-range",
                              type=float,
                              required=True,
                              nargs=3,
                              help="Start stop step for eps parameter")

    sweep_dbscan.add_argument("--min-samples-range",
                              type=int,
                              required=True,
                              nargs=3,
                              help="Start stop step for min_samples parameter")

    evaluate_parser = subparsers.add_parser("evaluate-predicted-genotypes",
                                            help="Evaluate predicted labels against known labels")

    evaluate_parser.add_argument("--predicted-labels-fl",
                                 type=str,
                                 required=True)

    evaluate_parser.add_argument("--known-labels-fl",
                                 type=str,
                                 required=True)

    return parser.parse_args()

if __name__ == "__main__":
    args = parseargs()

    if args.mode == "test-pcs":
        labels = read_labels(args.labels_fl)
        coordinates_fl = os.path.join(args.workdir,
                                      "pca_coordinates.tsv")
        sample_names, coordinates = read_pca_coordinates(coordinates_fl)

        test_pcs(coordinates,
                 sample_names,
                 labels)

    elif args.mode == "cluster":
        coordinates_fl = os.path.join(args.workdir,
                                      "pca_coordinates.tsv")
        sample_names, coordinates = read_pca_coordinates(coordinates_fl)

        cluster_samples(coordinates,
                        sample_names,
                        args.components,
                        args.n_clusters,
                        args.scale_features,
                        args.predicted_labels_fl)

    elif args.mode == "sweep-parameters":
        coordinates_fl = os.path.join(args.workdir,
                                      "pca_coordinates.tsv")
        sample_names, coordinates = read_pca_coordinates(coordinates_fl)
        known_labels = read_label_names(args.labels_fl)

        if args.sweep_mode == "kmeans":
            sweep_kmeans_parameters(coordinates,
                                    sample_names,
                                    known_labels,
                                    args.components,
                                    args.n_clusters)

        elif args.sweep_mode == "dbscan":
            sweep_dbscan_parameters(coordinates,
                                    sample_names,
                                    known_labels,
                                    args.components,
                                    args.eps_range,
                                    args.min_samples_range)

    elif args.mode == "evaluate-predicted-genotypes":
        evaluate_predictions(args.predicted_labels_fl,
                             args.known_labels_fl)

    else:
        print("Unknown mode '%s'" % args.mode)
        sys.exit(1)
